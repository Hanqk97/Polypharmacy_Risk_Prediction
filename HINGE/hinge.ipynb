{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Vocab Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hyperfacts(json_path):\n",
    "    \"\"\"\n",
    "    Reads hyperfacts from a JSON file, each entry like:\n",
    "      {\n",
    "        \"drug1\": \"DrugA\",\n",
    "        \"relation\": \"interactWith\",\n",
    "        \"drug2\": \"DrugC\",\n",
    "        \"attributes\": {\n",
    "            \"adverseEvent\": \"ConditionX\",\n",
    "            \"PRR\": 400.0\n",
    "        }\n",
    "      }\n",
    "    Returns a list of facts:\n",
    "      [\n",
    "        (h_str, r_str, t_str, [(k1_str, v1_str), (k2_str, v2_str), ...]),\n",
    "        ...\n",
    "      ]\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    facts = []\n",
    "    for entry in data:\n",
    "        h = entry[\"drug1\"]\n",
    "        r = entry[\"relation\"]\n",
    "        t = entry[\"drug2\"]\n",
    "        # Convert the \"attributes\" dict into a list of (k, v) pairs\n",
    "        kv_pairs = []\n",
    "        if \"attributes\" in entry:\n",
    "            for k, v in entry[\"attributes\"].items():\n",
    "                # v could be a string or numeric; if numeric, cast to string\n",
    "                # to keep everything consistent in entity embeddings\n",
    "                if not isinstance(v, str):\n",
    "                    v = str(v)\n",
    "                kv_pairs.append((k, v))\n",
    "        facts.append((h, r, t, kv_pairs))\n",
    "    return facts\n",
    "\n",
    "def load_conditions_from_json(conditions_path):\n",
    "    \"\"\"\n",
    "    Loads an array of condition strings from a JSON file.\n",
    "    E.g. [\"Dissociative disorder\", \"Incision site haemorrhage\", ...]\n",
    "    \"\"\"\n",
    "    with open(conditions_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def build_vocab_and_mappings(facts):\n",
    "    \"\"\"\n",
    "    Assigns unique IDs to each entity and relation found in the hyperfacts.\n",
    "    Returns:\n",
    "      entity2id (dict): maps entity string -> integer ID\n",
    "      relation2id (dict): maps relation string -> integer ID\n",
    "    \"\"\"\n",
    "    entity2id = {}\n",
    "    relation2id = {}\n",
    "    next_eid = 0\n",
    "    next_rid = 0\n",
    "\n",
    "    for (h, r, t, kv_pairs) in facts:\n",
    "        # Entities\n",
    "        if h not in entity2id:\n",
    "            entity2id[h] = next_eid\n",
    "            next_eid += 1\n",
    "        if t not in entity2id:\n",
    "            entity2id[t] = next_eid\n",
    "            next_eid += 1\n",
    "\n",
    "        # Relations\n",
    "        if r not in relation2id:\n",
    "            relation2id[r] = next_rid\n",
    "            next_rid += 1\n",
    "\n",
    "        # For the key-value pairs, we treat \"k\" like a relation; \"v\" like an entity.\n",
    "        for (k, v) in kv_pairs:\n",
    "            if k not in relation2id:\n",
    "                relation2id[k] = next_rid\n",
    "                next_rid += 1\n",
    "            if v not in entity2id:\n",
    "                entity2id[v] = next_eid\n",
    "                next_eid += 1\n",
    "\n",
    "    return entity2id, relation2id\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  HINGE Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HINGEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal PyTorch implementation of the HINGE architecture:\n",
    "      - triple-wise pipeline: (h, r, t)\n",
    "      - quintuple-wise pipeline: (h, r, t, k, v)\n",
    "      - merges features via elementwise MIN\n",
    "      - final linear layer for scoring\n",
    "    \"\"\"\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim=100, num_filters=400):\n",
    "        super(HINGEModel, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_filters = num_filters\n",
    "\n",
    "        # Lookup tables\n",
    "        self.entity_emb = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.relation_emb = nn.Embedding(num_relations, embedding_dim)\n",
    "\n",
    "        # CNN for triple (3 x embedding_dim)\n",
    "        # Filter size = (3,3) => a \"height\" of 3 to convolve across (h, r, t), \"width\" of 3\n",
    "        self.triple_conv = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0)\n",
    "        )\n",
    "\n",
    "        # CNN for quintuple (5 x embedding_dim)\n",
    "        # Filter size = (5,3)\n",
    "        self.quintuple_conv = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=(5, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0)\n",
    "        )\n",
    "\n",
    "        # After convolution + ReLU => shape is [batch_size, num_filters, 1, (embedding_dim - 2)]\n",
    "        # Flatten to [batch_size, num_filters*(embedding_dim-2)] => final linear\n",
    "        feature_dim = num_filters * (embedding_dim - 2)\n",
    "        self.proj = nn.Linear(feature_dim, 1)\n",
    "\n",
    "        # Weight init\n",
    "        nn.init.xavier_uniform_(self.entity_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.relation_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.triple_conv.weight)\n",
    "        nn.init.xavier_uniform_(self.quintuple_conv.weight)\n",
    "        nn.init.xavier_uniform_(self.proj.weight)\n",
    "\n",
    "    def triple_forward(self, h, r, t):\n",
    "        \"\"\"\n",
    "        Forward pass for triple (h, r, t).\n",
    "        Input shapes: h, r, t => [batch_size] of IDs\n",
    "        Returns: a feature tensor => [batch_size, num_filters*(embedding_dim-2)]\n",
    "        \"\"\"\n",
    "        h_emb = self.entity_emb(h)   # => [batch_size, embedding_dim]\n",
    "        r_emb = self.relation_emb(r)\n",
    "        t_emb = self.entity_emb(t)\n",
    "\n",
    "        # Stack in a \"height\" dimension => [batch_size, 3, embedding_dim]\n",
    "        x = torch.stack([h_emb, r_emb, t_emb], dim=1)  # shape [B, 3, E]\n",
    "        # Unsqueeze for conv2d => [B, 1, 3, E]\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = self.triple_conv(x)  # => [B, num_filters, 3-3+1=1, E-3+1=E-2]\n",
    "        x = F.relu(x)\n",
    "        # Squeeze the height dimension => [B, num_filters, embedding_dim-2]\n",
    "        x = x.squeeze(2)  # remove dimension=2 if it is size 1\n",
    "        # Flatten => [B, num_filters*(embedding_dim-2)]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "    def quintuple_forward(self, h, r, t, k, v):\n",
    "        \"\"\"\n",
    "        Forward pass for quintuple (h, r, t, k, v).\n",
    "        \"\"\"\n",
    "        h_emb = self.entity_emb(h)\n",
    "        r_emb = self.relation_emb(r)\n",
    "        t_emb = self.entity_emb(t)\n",
    "        k_emb = self.relation_emb(k)\n",
    "        v_emb = self.entity_emb(v)\n",
    "\n",
    "        # shape [B, 5, E]\n",
    "        x = torch.stack([h_emb, r_emb, t_emb, k_emb, v_emb], dim=1)\n",
    "        # => [B, 1, 5, E]\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = self.quintuple_conv(x)  # => [B, num_filters, 1, E-2]\n",
    "        x = F.relu(x)\n",
    "        x = x.squeeze(2)           # => [B, num_filters, E-2]\n",
    "        x = x.view(x.size(0), -1)  # => [B, num_filters*(E-2)]\n",
    "        return x\n",
    "\n",
    "    def forward(self, h, r, t, kv_pairs):\n",
    "        \"\"\"\n",
    "        Forward pass for a single hyper-relational fact with N key-value pairs.\n",
    "        h, r, t => [batch_size] of size 1 if we handle 1 fact at a time\n",
    "        kv_pairs => list of (k_id, v_id), each shape [batch_size] (or scalar).\n",
    "\n",
    "        Steps:\n",
    "          1) get triple_feat\n",
    "          2) get quintuple_feat for each (k, v)\n",
    "          3) combine via elementwise min along feature dimension\n",
    "          4) project => score\n",
    "        \"\"\"\n",
    "        # triple-wise features: shape [B, F]\n",
    "        triple_feat = self.triple_forward(h, r, t)\n",
    "\n",
    "        if len(kv_pairs) == 0:\n",
    "            # If no attributes, the final feature is just triple_feat\n",
    "            merged_feat = triple_feat\n",
    "        else:\n",
    "            # For each (k, v), get quintuple features => shape [B, F]\n",
    "            # We'll store them in a list => then stack => shape [N, B, F]\n",
    "            quint_feat_list = []\n",
    "            for (k_id, v_id) in kv_pairs:\n",
    "                qf = self.quintuple_forward(h, r, t, k_id, v_id)  # => [B, F]\n",
    "                quint_feat_list.append(qf)\n",
    "            # Stack => [N, B, F]\n",
    "            qfeats = torch.stack(quint_feat_list, dim=0)\n",
    "\n",
    "            # We want to do elementwise min along dimension=0 *and* incorporate triple_feat\n",
    "            # But we need triple_feat repeated N times (or we can do pairwise min per step).\n",
    "            # A simpler approach is:\n",
    "            # 1) Expand triple_feat => shape [N, B, F]\n",
    "            triple_feat_expanded = triple_feat.unsqueeze(0).expand(qfeats.size(0), -1, -1)\n",
    "            # 2) min => shape [N, B, F]\n",
    "            combined = torch.min(triple_feat_expanded, qfeats)\n",
    "            # 3) Now we min across N => shape [B, F]\n",
    "            merged_feat, _ = torch.min(combined, dim=0)\n",
    "\n",
    "        # final projection => [B, 1]\n",
    "        score = self.proj(merged_feat)\n",
    "        return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage (training skeleton + top-K condition inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num entities: 4605, Num relations: 3\n",
      "Epoch 0, total_loss = 12913.0746\n",
      "\n",
      "Predicted hyper-relations for (Temazepam, sildenafil):\n",
      "  - Temazepam + sildenafil -> Tremor (Scoring: 2.2029)\n",
      "  - Temazepam + sildenafil -> Drug interaction (Scoring: 2.1987)\n",
      "  - Temazepam + sildenafil -> Tachycardia (Scoring: 2.1964)\n",
      "  - Temazepam + sildenafil -> Abdominal pain (Scoring: 2.1925)\n",
      "  - Temazepam + sildenafil -> Thrombocytopenia (Scoring: 2.1908)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # ------------------------------------------------\n",
    "    # A) Load data\n",
    "    # ------------------------------------------------\n",
    "    hyperfacts_path = \"hyperfacts.json\"   # your hyperfacts\n",
    "    conditions_path = \"conditions.json\"   # your array of condition strings\n",
    "    facts = load_hyperfacts(hyperfacts_path)\n",
    "    all_conditions = load_conditions_from_json(conditions_path)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # B) Build vocab\n",
    "    # ------------------------------------------------\n",
    "    entity2id, relation2id = build_vocab_and_mappings(facts)\n",
    "    print(f\"Num entities: {len(entity2id)}, Num relations: {len(relation2id)}\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # C) Instantiate model\n",
    "    # ------------------------------------------------\n",
    "    model = HINGEModel(num_entities=len(entity2id),\n",
    "                       num_relations=len(relation2id),\n",
    "                       embedding_dim=100,\n",
    "                       num_filters=400)\n",
    "\n",
    "    # Typically define an optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # D) Training Skeleton (not a full example)\n",
    "    # ------------------------------------------------\n",
    "    model.train()\n",
    "\n",
    "    # For demonstration, we do one epoch with a simple negative sampling approach\n",
    "    # This is not a full robust training loop, just a placeholder:\n",
    "    for epoch in range(1):\n",
    "        total_loss = 0.0\n",
    "        for (h_str, r_str, t_str, kv_strs) in facts:\n",
    "            # Convert strings to IDs\n",
    "            h_id = torch.tensor([entity2id[h_str]], dtype=torch.long)\n",
    "            r_id = torch.tensor([relation2id[r_str]], dtype=torch.long)\n",
    "            t_id = torch.tensor([entity2id[t_str]], dtype=torch.long)\n",
    "\n",
    "            kv_pairs_ids = []\n",
    "            for (k_str, v_str) in kv_strs:\n",
    "                k_id = torch.tensor([relation2id[k_str]], dtype=torch.long)\n",
    "                v_id = torch.tensor([entity2id[v_str]], dtype=torch.long)\n",
    "                kv_pairs_ids.append((k_id, v_id))\n",
    "\n",
    "            # 1) Positive score\n",
    "            pos_score = model(h_id, r_id, t_id, kv_pairs_ids)\n",
    "\n",
    "            # 2) Sample a negative fact by corrupting one entity or relation\n",
    "            # (Simplified; you might randomly choose which to corrupt)\n",
    "            # e.g., randomly pick an entity from entity2id\n",
    "            import random\n",
    "            do_corrupt_entity = random.random() < 0.5\n",
    "            if do_corrupt_entity:\n",
    "                # Corrupt the tail entity\n",
    "                neg_t_str = random.choice(list(entity2id.keys()))\n",
    "                neg_t_id = torch.tensor([entity2id[neg_t_str]], dtype=torch.long)\n",
    "                neg_score = model(h_id, r_id, neg_t_id, kv_pairs_ids)\n",
    "            else:\n",
    "                # Corrupt the relation\n",
    "                neg_r_str = random.choice(list(relation2id.keys()))\n",
    "                neg_r_id = torch.tensor([relation2id[neg_r_str]], dtype=torch.long)\n",
    "                neg_score = model(h_id, neg_r_id, t_id, kv_pairs_ids)\n",
    "\n",
    "            # 3) Softplus loss => sum( log(1 + exp(-pos_score)) + log(1 + exp(neg_score)) )\n",
    "            # or margin-based, etc.\n",
    "            loss = F.softplus(-pos_score) + F.softplus(neg_score)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch}, total_loss = {total_loss:.4f}\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # E) Inference: Predict top-K conditions\n",
    "    # ------------------------------------------------\n",
    "    model.eval()\n",
    "\n",
    "    # Example query: we want conditions for (DrugA, interactWith, DrugC)\n",
    "    query_drugA = \"Temazepam\"\n",
    "    query_drugC = \"sildenafil\"\n",
    "    query_relation = \"interactWith\"\n",
    "\n",
    "    if (query_drugA not in entity2id) or (query_drugC not in entity2id) or (query_relation not in relation2id):\n",
    "        print(\"One of these is not in the vocab. Please fix or handle OOV.\")\n",
    "        return\n",
    "\n",
    "    h_id = torch.tensor([entity2id[query_drugA]], dtype=torch.long)\n",
    "    r_id = torch.tensor([relation2id[query_relation]], dtype=torch.long)\n",
    "    t_id = torch.tensor([entity2id[query_drugC]], dtype=torch.long)\n",
    "\n",
    "    # We'll treat each condition as a potential (k, v) pair => (adverseEvent, conditionX)\n",
    "    attribute_key = \"adverseEvent\"\n",
    "    if attribute_key not in relation2id:\n",
    "        print(f\"Relation '{attribute_key}' not in vocab. Please fix.\")\n",
    "        return\n",
    "    key_id = torch.tensor([relation2id[attribute_key]], dtype=torch.long)\n",
    "\n",
    "    candidate_scores = []\n",
    "    with torch.no_grad():\n",
    "        for cond_str in all_conditions:\n",
    "            # If this condition is not in the entity vocab, skip or use an OOV approach\n",
    "            if cond_str not in entity2id:\n",
    "                continue\n",
    "            cond_id = torch.tensor([entity2id[cond_str]], dtype=torch.long)\n",
    "\n",
    "            # Evaluate the hyperfact: (DrugA, interactWith, DrugC) with attribute (adverseEvent -> cond)\n",
    "            # i.e. one kv_pair => (key_id, cond_id)\n",
    "            score = model(h_id, r_id, t_id, [(key_id, cond_id)])\n",
    "            candidate_scores.append((cond_str, score.item()))\n",
    "\n",
    "    # Sort by descending score\n",
    "    candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Show top 3\n",
    "    K = 5\n",
    "    top_k = candidate_scores[:K]\n",
    "\n",
    "    print(f\"\\nPredicted hyper-relations for ({query_drugA}, {query_drugC}):\")\n",
    "    for cond_str, sc in top_k:\n",
    "        print(f\"  - {query_drugA} + {query_drugC} -> {cond_str} (Scoring: {sc:.4f})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num entities: 4605, Num relations: 3\n",
      "Epoch 0, total_loss = 12780.0887\n",
      "\n",
      "Predicted hyper-relations for (Temazepam, Prednisone):\n",
      "  - Temazepam + Prednisone -> Dizziness (Scoring: 2.2133)\n",
      "  - Temazepam + Prednisone -> Chest pain (Scoring: 2.2014)\n",
      "  - Temazepam + Prednisone -> Pneumonia (Scoring: 2.2001)\n",
      "  - Temazepam + Prednisone -> Pain (Scoring: 2.1941)\n",
      "  - Temazepam + Prednisone -> Muscle spasms (Scoring: 2.1936)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # ------------------------------------------------\n",
    "    # A) Load data\n",
    "    # ------------------------------------------------\n",
    "    hyperfacts_path = \"hyperfacts.json\"   # your hyperfacts\n",
    "    conditions_path = \"conditions.json\"   # your array of condition strings\n",
    "    facts = load_hyperfacts(hyperfacts_path)\n",
    "    all_conditions = load_conditions_from_json(conditions_path)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # B) Build vocab\n",
    "    # ------------------------------------------------\n",
    "    entity2id, relation2id = build_vocab_and_mappings(facts)\n",
    "    print(f\"Num entities: {len(entity2id)}, Num relations: {len(relation2id)}\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # C) Instantiate model\n",
    "    # ------------------------------------------------\n",
    "    model = HINGEModel(num_entities=len(entity2id),\n",
    "                       num_relations=len(relation2id),\n",
    "                       embedding_dim=100,\n",
    "                       num_filters=400)\n",
    "\n",
    "    # Typically define an optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # D) Training Skeleton (not a full example)\n",
    "    # ------------------------------------------------\n",
    "    model.train()\n",
    "\n",
    "    # For demonstration, we do one epoch with a simple negative sampling approach\n",
    "    # This is not a full robust training loop, just a placeholder:\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0.0\n",
    "        for (h_str, r_str, t_str, kv_strs) in facts:\n",
    "            # Convert strings to IDs\n",
    "            h_id = torch.tensor([entity2id[h_str]], dtype=torch.long)\n",
    "            r_id = torch.tensor([relation2id[r_str]], dtype=torch.long)\n",
    "            t_id = torch.tensor([entity2id[t_str]], dtype=torch.long)\n",
    "\n",
    "            kv_pairs_ids = []\n",
    "            for (k_str, v_str) in kv_strs:\n",
    "                k_id = torch.tensor([relation2id[k_str]], dtype=torch.long)\n",
    "                v_id = torch.tensor([entity2id[v_str]], dtype=torch.long)\n",
    "                kv_pairs_ids.append((k_id, v_id))\n",
    "\n",
    "            # 1) Positive score\n",
    "            pos_score = model(h_id, r_id, t_id, kv_pairs_ids)\n",
    "\n",
    "            # 2) Sample a negative fact by corrupting one entity or relation\n",
    "            # (Simplified; you might randomly choose which to corrupt)\n",
    "            # e.g., randomly pick an entity from entity2id\n",
    "            import random\n",
    "            do_corrupt_entity = random.random() < 0.5\n",
    "            if do_corrupt_entity:\n",
    "                # Corrupt the tail entity\n",
    "                neg_t_str = random.choice(list(entity2id.keys()))\n",
    "                neg_t_id = torch.tensor([entity2id[neg_t_str]], dtype=torch.long)\n",
    "                neg_score = model(h_id, r_id, neg_t_id, kv_pairs_ids)\n",
    "            else:\n",
    "                # Corrupt the relation\n",
    "                neg_r_str = random.choice(list(relation2id.keys()))\n",
    "                neg_r_id = torch.tensor([relation2id[neg_r_str]], dtype=torch.long)\n",
    "                neg_score = model(h_id, neg_r_id, t_id, kv_pairs_ids)\n",
    "\n",
    "            # 3) Softplus loss => sum( log(1 + exp(-pos_score)) + log(1 + exp(neg_score)) )\n",
    "            # or margin-based, etc.\n",
    "            loss = F.softplus(-pos_score) + F.softplus(neg_score)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch}, total_loss = {total_loss:.4f}\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # E) Inference: Predict top-K conditions\n",
    "    # ------------------------------------------------\n",
    "    model.eval()\n",
    "\n",
    "    # Example query: we want conditions for (DrugA, interactWith, DrugC)\n",
    "    query_drugA = \"Temazepam\"\n",
    "    query_drugC = \"Prednisone\"\n",
    "    query_relation = \"interactWith\"\n",
    "\n",
    "    if (query_drugA not in entity2id) or (query_drugC not in entity2id) or (query_relation not in relation2id):\n",
    "        print(\"One of these is not in the vocab. Please fix or handle OOV.\")\n",
    "        return\n",
    "\n",
    "    h_id = torch.tensor([entity2id[query_drugA]], dtype=torch.long)\n",
    "    r_id = torch.tensor([relation2id[query_relation]], dtype=torch.long)\n",
    "    t_id = torch.tensor([entity2id[query_drugC]], dtype=torch.long)\n",
    "\n",
    "    # We'll treat each condition as a potential (k, v) pair => (adverseEvent, conditionX)\n",
    "    attribute_key = \"adverseEvent\"\n",
    "    if attribute_key not in relation2id:\n",
    "        print(f\"Relation '{attribute_key}' not in vocab. Please fix.\")\n",
    "        return\n",
    "    key_id = torch.tensor([relation2id[attribute_key]], dtype=torch.long)\n",
    "\n",
    "    candidate_scores = []\n",
    "    with torch.no_grad():\n",
    "        for cond_str in all_conditions:\n",
    "            # If this condition is not in the entity vocab, skip or use an OOV approach\n",
    "            if cond_str not in entity2id:\n",
    "                continue\n",
    "            cond_id = torch.tensor([entity2id[cond_str]], dtype=torch.long)\n",
    "\n",
    "            # Evaluate the hyperfact: (DrugA, interactWith, DrugC) with attribute (adverseEvent -> cond)\n",
    "            # i.e. one kv_pair => (key_id, cond_id)\n",
    "            score = model(h_id, r_id, t_id, [(key_id, cond_id)])\n",
    "            candidate_scores.append((cond_str, score.item()))\n",
    "\n",
    "    # Sort by descending score\n",
    "    candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Show top 3\n",
    "    K = 5\n",
    "    top_k = candidate_scores[:K]\n",
    "\n",
    "    print(f\"\\nPredicted hyper-relations for ({query_drugA}, {query_drugC}):\")\n",
    "    for cond_str, sc in top_k:\n",
    "        print(f\"  - {query_drugA} + {query_drugC} -> {cond_str} (Scoring: {sc:.4f})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polypharmacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
